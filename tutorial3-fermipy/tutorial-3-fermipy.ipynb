{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Fermipy\n",
    "\n",
    "There is a Python library named ***fermipy*** that was developed at NASA Goddard by (mainly) Matt Wood with the help of several others. It is very good at simplifying the whole *Fermi* analysis chain - as we will demonstrate shortly. \n",
    "\n",
    "The following tutorial should take place inside the included directory called \"NGC1275_fermipy\". If you're using the Durham tutorial docker image, you'll find all the analysis files inside your locally mounted directory i.e. ```/workdir/tutorial3-fermipy/NGC1275_fermipy```. As we step through the tutorial this directory will be created below and for the sake of this tutorial we'll assume you're using the docker image. If not, for example you're using your own installation of ***fermipy***, you may need to adjust all paths accordingly.\n",
    "\n",
    "## Intro\n",
    "\n",
    "Analysis using ***fermipy*** requires a configuration file in the YAML format, which is then used to run the whole analysis chain along with optimized fitting routines and several other bells and whistles to make your analysis **significantly** easier. Firstly let's have a look at the default configuration, which has all of the different possible parameters in it for your perusal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, lets check the current directory..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = %pwd\n",
    "print(current_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is just for some housekeeping to ensure the analysis works within the Docker image directory structure we've set up. Lets just check the output of the PFILES environment variable..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Step 1: Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "print(f\" Current directory = {current_dir}\")\n",
    "\n",
    "# Step 2: Get the existing PFILES environment variable (if it exists)\n",
    "pfiles_dir = os.getenv('PFILES','')\n",
    "print(f\" PFILES env. var. = {pfiles_dir}\")\n",
    "\n",
    "# Step 3: Append the current directory to PFILES\n",
    "if pfiles_dir:\n",
    "    # If PFILES is already set, append the current directory\n",
    "    new_pfiles = f\"{current_dir};{pfiles_dir}\"\n",
    "else:\n",
    "    # If PFILES is not set, just use the current directory\n",
    "    new_pfiles = current_dir\n",
    "\n",
    "# Step 4: Update the PFILES environment variable\n",
    "os.environ['PFILES'] = new_pfiles\n",
    "\n",
    "# Step 5: Print the new PFILES value for verification\n",
    "print(\"Updated PFILES:\", os.environ['PFILES'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's list all the files in the current directory..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lhtr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, finally we're ready to start analysing using ***fermipy***. Firstly we instantiate the GTAnalysis class into a new object called \"gta\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fermipy.gtanalysis import GTAnalysis\n",
    "\n",
    "# Create an analysis object from the config file\n",
    "gta = GTAnalysis('config.yaml', logging={'verbosity': 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can normally ignore any 'warning' messages, they're generally benign and usually are there to inform you of possible deprecations in future releases for example. Now we'll run our setup.\n",
    "\n",
    "Note: The next step will take about 10 minutes to complete so you'll need to be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gta.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can immediately see how powerful ***fermipy*** is, based on the output (in the red textbox) of this one command. Given the parameters we defined in the config **YAML** file, the setup function has run *gtselect*, and filtered our photon files in the same way that the command line tool would have, by ROI, time, event class and event type. It has then created a livetime cube, using *gtltcube* (actually, it skipped this step, because we already created it for you, to save you some time). It then ran *gtbin* to bin our photons, *gtexpcube2* to compute exposure, and *gtsrcmaps*, to generate a source map of our ROI. \n",
    "\n",
    "This is all exceptionally useful, as this allows us to essentially run all of the science tools from Tutorial 2, in one command, based on our input from the configuration file. It saves us time and effort, and reduces the odds of human error! Nice!\n",
    "\n",
    "Now we've got our model set up, we should try and fit it to our data. The first method we can use is to optimise the fit. \n",
    "\n",
    "There are several methods of statistically fitting models, $ \\chi ^2$ minimisation and linear regression being two you have probably encountered the most. As our data is multidimensional (in space, time and energy), we use a more suitable approach, as described in [Mattox et al (1996)](https://ui.adsabs.harvard.edu/abs/1996ApJ...461..396M/abstract), called Maximum Likelihood Estimation (MLE). \n",
    "\n",
    "Each source in our model has different parameters, with associated statistical likelihoods. By varying the parameters to maximise the likelihoods, we improve the fit of our model. This method is particularly suitable for our data and modelling. You don't need to know in great depth about MLE to do *Fermi* analysis, however it really gives you a greater appreciation of what you're doing mathematically. We'll leave it to you to research this for yourselves.\n",
    "\n",
    "The first thing we should do with our model is optimise it. Optimisation is a ***fermipy*** routine which iteratively pushes the parameters of each source close to their global maxima, and thus gives us a reasonable fit for our model to the data.\n",
    "\n",
    "Run the cell below to run the optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gta.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go! Our optimization is finished and we should now have a model which represents the data in a much better way than before. But can we do better?\n",
    "\n",
    "HINT: When conducting ***fermipy*** analysis, you should always run the optimisation step at least once.\n",
    "\n",
    "As mentioned before, each source has a set of parameters e.g. normalisation of the spectral power law, and spectral shape etc. By freeing these parameters, we can execute a fit with respect to these. A fit provides a much better, well, fit, of the model to the data with respect to the freed parameters, however is very time consuming for our machines to do. In the case of NGC 1275 (which we are looking at) we probably want to fit the normalisation, and the normalisation of the sources around it. Parameters like spectral shape can be left alone, because we are pretty convinced that it is a point source, and there is no evidence for extended $\\gamma$-ray emission.\n",
    "\n",
    "First, we free the normalisation for a 3 degree region centered on NGC 1275, and also our background components in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gta.free_sources(distance=3.0,pars='norm') #frees the point sources\n",
    "gta.free_source('galdiff', pars='norm') #frees the galactic diffuse\n",
    "gta.free_source('isodiff', pars='norm') #frees the isotropic diffuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have defined the sources and which of their parameters are free to vary, we can execute our fit with this simple command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gta.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and we're done!\n",
    "\n",
    "Using ***fermipy*** we've successfully built a model using the Science Tools, and fitted it to our data in a few simple lines of code. Now we've done our modelling, we should really check how accurate it is. Fortunately, there is an easy way to do this, called a residual map. Our data shows the number of photons in each bin, and our model tries to predict this, so by subtracting one from the other on a bin by bin basis, we are able to see how well our model predicts the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gta.residmap(make_plots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we included the argument ```make_plots=True``` in our command, ***fermipy*** has automatically generated some plots of the residual maps for us to look at. Generally these 'diagnostic plots' are of poor quality and we wouldn't consider them for publication. While the diagnostic plots are extremely useful, these can sometimes (for example SED)be wrong, and the important thing to remember is that these are diagnostic plots and should always be treated as such! To assess the quality of our model let us inspect our residual maps:\n",
    "\n",
    "First we can look at our data itself:\n",
    "\n",
    "![data_counts](../img/data_counts.png)\n",
    "\n",
    "The big bright blob in the centre is our galaxy, NGC 1275 (or 4FGL J0319.8+4130 in the 4th *Fermi*-LAT point source catalog). We can see there are some other, less bright sources nearby, and a large amount of diffuse emission in the top left of the map, this is from the Galactic plane! Clearly though NGC 1275 is the brightest thing in our ROI, which is useful (although not always the case depending on what you're looking at). \n",
    "\n",
    "Now though, we can look at our model:\n",
    "\n",
    "![model_counts](../img/model_counts.png)\n",
    "\n",
    "The two maps are almost indistinguishable from one another, which is good! We want our modelling to be as representative of our data as possible. Now subtracting the two... \n",
    "\n",
    "![rmap_counts](../img/rmap_counts.png)\n",
    "\n",
    "We can see that across the ROI, we have an excess of counts (the red), which indicates that we are undermodelling the data (i.e. not predicting enough counts). But right in the centre, we have the opposite problem of overmodelling. To see how much of an issue this is, we need to turn these number of counts into something more statistically meaningful, and we do this representing the residuals in significance space:\n",
    "\n",
    "![rmap_sigma](../img/rmap_sigma.png)\n",
    "\n",
    "Now we see that our overmodelling and undermodelling are not very statistically significant, and thus we can be confident that our ROI is modelled well.\n",
    "\n",
    "HINT: Due to the complexity of modelling the diffuse $\\gamma$-ray background, statistical fluctuations of $2-3\\sigma$ are not uncommon, and hence why the accepted detection threshold for the discovery of a new $\\gamma$-ray source is $5 \\sigma$\n",
    "\n",
    "Examining our maps, we should notice that all of our sources in the model are from the 4FGL. This is because when we built the model, ***fermipy*** used the 4FGL to automatically insert all of the known 4FGL sources into our model. It would however be naive to assume that this is the most accurate representation of our ROI. One major issue is sources that may not be catalogued. Luckily, we have a method to deal with that. First, we will generate a TS map of our ROI. The TS (test statistic) is a statistic which arises from our likelihood modelling, but directly correlates to a significance value for $k$ degrees of freedom between hypotheses. For one degree of freedom however, the square root of the TS gives us our significance value, so a TS of 9 is $3 \\sigma$ etc. When we generate a TS map, it will give us the significance of any unmodelled emission across the ROI, and thus let us see if there is any emission left to be modelled!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gta.tsmap(make_plots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now open and inspect our generated TS map (in significance) we can see there are a few hot spots of emission, but nothing too much to worry about. \n",
    "\n",
    "![TS_sigma](../img/TS_sigma.png)\n",
    "\n",
    "To be safe, we can run a source finding algorithm, which will fit a point source to any position with a significance $> 4 \\sigma$. It does like to take a few minutes though, so use this opportunity to go and get a coffee/beer depending on whether its before/after 4pm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gta.find_sources(sqrt_ts_threshold=4.0, min_separation=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're done! If we now generate a new TS map, we'll see.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gta.tsmap(make_plots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... we now have some more sources added! That is very handy and should account for some of the errant emission in our ROI. \n",
    "\n",
    "![TS_sigma2](../img/TS_sigma2.png)\n",
    "\n",
    "Now we are absolutely 100% certain our ROI is as good as it can be (to be honest, we might have gone slightly overboard, but we wanted to show you all the features of ***fermipy***). Anyway, now that we have got this far it would be extremely annoying if we were to lose all of this work! So now is a good time to tidy up our ROI and save it. When we say tidy up, we mean deleting all of our low significance sources. Just because there is a catalogued source in the 4FGL, this doesn't mean it is significant in our ROI, so we delete anything of significance $ < 3 \\sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "gta.delete_sources(minmax_ts=[-np.inf, 9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we will save our ROI, and generate some plots while we do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gta.write_roi('NGC_1275_fit', make_plots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load an ROI back in with ```gta.load('NGC_1275_fit')``` if we need to, after defining gta again.\n",
    "\n",
    "You can feel free to peruse these plots at your own leisure, but for now, we will move on to some more advanced features of ***fermipy*** for source analysis.\n",
    "\n",
    "The first thing we normally want to do, is to calculate a spectral energy distribution (SED) of the source. This shows the relationship between the flux and the energy of the source. Thankfully ***fermipy*** can do this for us quite easily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sed = gta.sed('4FGL J0319.8+4130') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, the diagnostic plots for SEDs are often flat out incorrect, which means we need to plot our own. So lets do that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "ts = sed['ts']\n",
    "dnde = sed['dnde']\n",
    "flux_error = sed['e2dnde_err']\n",
    "e_min = sed['e_min']\n",
    "e_ref = sed['e_ref']\n",
    "e_max = sed['e_max']\n",
    "uplim = sed['e2dnde_ul95']\n",
    "\n",
    "flux = (e_ref**2)*dnde\n",
    "yval = []\n",
    "ts_int = []\n",
    "uplim_bool = []\n",
    "i = 0\n",
    "i_array = []\n",
    "\n",
    "\n",
    "while i < len(ts):\n",
    "    ts_int.append(int(ts[i]))\n",
    "    if ts_int[i] < 4:\n",
    "        yval.append(uplim[i])\n",
    "        uplim_bool.append(1)\n",
    "        \n",
    "    else:\n",
    "        yval.append(flux[i])\n",
    "        uplim_bool.append(0)\n",
    "        i_array.append(i)\n",
    "    i = i + 1\n",
    "\n",
    "uplims = np.array(uplim_bool, dtype=bool)\n",
    "\n",
    "low_xerr = []\n",
    "high_xerr = []\n",
    "j = 0\n",
    "\n",
    "flux_valid = []\n",
    "E_ctr_valid = []\n",
    "for m in i_array:\n",
    "    flux_valid.append(flux[m])\n",
    "    E_ctr_valid.append(e_ref[m])\n",
    "\n",
    "\n",
    "while j < len(e_ref):\n",
    "    a = e_ref[j] - e_min[j]\n",
    "    b = e_max[j] - e_ref[j]\n",
    "    low_xerr.append(a)\n",
    "    high_xerr.append(b)\n",
    "    j = j + 1\n",
    "\n",
    "xerr = [low_xerr, high_xerr]\n",
    "\n",
    "fig = plt.subplots(figsize=(14, 8))\n",
    "plt.errorbar(e_ref, uplim, xerr = xerr, yerr = flux_error\n",
    "             , uplims = uplims, linestyle = 'None',\n",
    "             color = 'black', marker = 'o')\n",
    "\n",
    "\n",
    "plt.title('NGC 1275 Spectral Energy Distribution')\n",
    "plt.xlabel('Energy [MeV]')\n",
    "plt.ylabel('Flux MeV cm^(-2) s^(-1)')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.savefig('sed_NGC_1275.png', dpi = 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've plotted our SED, lets take a look (it may or may not have shown up in the output of the above box). \n",
    "\n",
    "![SED](../img/sed_NGC_1275.png)\n",
    "\n",
    "That looks great! our code has plotted our SED, with error bars and energy bin widths, and plotted 95% confidence upper limits on bins where $ \\sigma < 2$. We could, of course plot these points too, but low significance bins have large error bars, and we aren't certain enough in them to take them seriously in any fit. We haven't fitted a line to our data, although if we wanted to, we could use $\\chi ^2$ minimisation to fit a log parabola to it. \n",
    "\n",
    "You are more than welcome to plagiarise this code for plotting, although it is actually pretty old and inelegant (it was written for an old analysis of the Crab Nebula). However, it does work and thats what matters!\n",
    "\n",
    "Now we know that our SED has a log parabolic shape, and we can see the distribution of flux with energy, what else can we do? \n",
    "\n",
    "So far we've been testing NGC 1275 as if it were a point source, however extension in *Fermi* sources does exist (although it is uncommon). Two radio galaxies are known to have extension: Centaurus A (our closest AGN neighbour) and Fornax A. Luckily ***fermipy*** has a lovely routine called 'extension' to test whether a source is a point source, or is extended. Lets run it now and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = gta.extension('4FGL J0319.8+4130')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've computed the extension of the source, lets take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext['ts_ext']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a great deal of information in the numpy array produced by the extension algorithm. Luckily however, we are given a test statistic of extension. This test statistic gives us an indication as to how extended a source is. Remember, the square root of the TS is the significance! Thus our threshold for claiming extension is $TS = 25$ or $5 \\sigma$. Our TS of extension is 0.05(ish) is far below this, and thus our hypothesis that NGC 1275 is a point source, remains correct. \n",
    "\n",
    "We can also look at the other parameters from the extension fit, but this is rather pointless considering that the source isn't extended! We'll leave it to your curiosity to explore. \n",
    "\n",
    "There are two other useful tools we'll explore with ***fermipy*** for point source analysis. The first is localisation, which iteratively tests the position of our source in a whole load of different places, thus shifting our source position to the most likely place in the model. When we add our sources to the model from the catalogue, it takes the RA and Dec from the catalogued position. However, this might not always be the most accurate. In particular, we can use localisation to improve the positional accuracy of sources added with the find sources algorithm. Lets give it a go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = gta.localize('4FGL J0319.8+4130')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're done. Lets take a look inside the numpy array to see our new position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc['ra'], loc['dec']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are our best fit coordinates for NGC 1275, given our year of data. We can also see that they really aren't dramatically different to what they originally were. In fact, it would be very surprising and slightly worrying if they were!\n",
    "\n",
    "Now on to the final key bit of point source analysis: the light-curve. Light-curves are one of the most useful, and powerful tools in an astronomers arsenal, an easy way to probe whether the flux of a source varies in time. Unfortunately, in ***fermipy***, they take a long time to make, due to the fact we need to compute the livetime in each bin. The command to produce a light-curve is slightly more complex than just putting in the name of our source, so we will go through these together now. \n",
    "\n",
    "First, we have the name of our source (obviously). Then we specify the number of time bins for our light-curve using the ```nbins``` parameter. As we have 1 year of data, we can look at the flux in monthly bins. This is a sensible timescale, and with most objects it is difficult to see anything at all with smaller bins. The ```multithread``` option allows our lightcurve algorithm to split up its jobs between the separate cores on our computer, thus allowing us to simultaneously compute several bins at once. This has the effect of speeding up our lightcurve by a factor of 2-4, with the side effect of essentially bricking our computer until its finished. We can choose to use scaled source maps by setting the ```use_scaled_srcmap``` parameter to true. This allows us to gain a huge advantage in computing speeds, by sacrificing a little accuracy in our modelling. This loss of accuracy is normally negligible, especially for an off the plane source like NGC 1275. The ```save_bin_data``` parameter allows the saving of bin data in individual folders, which leads to more reliable lightcurve generation when multithreading is on, and we highly recommend you enable this.\n",
    "\n",
    "Lets run it and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc = gta.lightcurve('4FGL J0319.8+4130', nbins=12, multithread=True,\n",
    "                    use_scaled_srcmap=True, save_bin_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gta.lightcurve will not produce diagnostic plots, so we need to plot these ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "lc=np.load('4fgl_j0319.8+4130_lightcurve.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ts_1 = lc[()]['ts']\n",
    "tmin_1 = lc[()]['tmin_mjd']\n",
    "tmax_1 = lc[()]['tmax_mjd']\n",
    "flux_1 = lc[()]['eflux']\n",
    "ul_1 = lc[()]['eflux_ul95']\n",
    "flux_err_1 = lc[()]['eflux_err']\n",
    "\n",
    "tcen = []\n",
    "i = 0\n",
    "\n",
    "ts = []\n",
    "tmin = []\n",
    "tmax = []\n",
    "flux = []\n",
    "ul = []\n",
    "flux_err = []\n",
    "i = 0\n",
    "while i < len(ts_1):\n",
    "    ts.append(ts_1[i])\n",
    "    tmin.append(tmin_1[i])\n",
    "    tmax.append(tmax_1[i])\n",
    "    flux.append(flux_1[i])\n",
    "    ul.append(ul_1[i])\n",
    "    flux_err.append(flux_err_1[i])\n",
    "    i = i + 1\n",
    "\n",
    "\n",
    "where_are_NaNs = np.isnan(ts)\n",
    "i = 0\n",
    "while i < len(where_are_NaNs):\n",
    "    if where_are_NaNs[i]==True:\n",
    "        ts[i] = 0\n",
    "        tmin[i] = 0\n",
    "        tmax[i] = 0\n",
    "        flux[i] = 0\n",
    "        ul[i] = 0\n",
    "        flux_err[i] = 0\n",
    "        i = i + 1\n",
    "    else:\n",
    "        i = i + 1\n",
    "\n",
    "i = 0\n",
    "while i < len(ts):\n",
    "    if ts[i] == 0:\n",
    "        ts.remove(ts[i])\n",
    "        tmin.remove(tmin[i])\n",
    "        tmax.remove(tmax[i])\n",
    "        flux.remove(flux[i])\n",
    "        ul.remove(ul[i])\n",
    "        flux_err.remove(flux_err[i])\n",
    "        i = i - 1\n",
    "    i = i + 1\n",
    "\n",
    "i = 0\n",
    "while i < len(tmin):\n",
    "    t = (tmin[i] + tmax[i])/2\n",
    "    tcen.append(t)\n",
    "    i = i + 1\n",
    "\n",
    "flux_plot = []\n",
    "flux_err_plot = []\n",
    "ts_int = []\n",
    "uplim_bool = []\n",
    "j = 0\n",
    "\n",
    "while j < len(ts):\n",
    "    ts_int.append(int(ts[j]))\n",
    "    \n",
    "    if ts_int[j] < 4:                ### CHANGE TS THRESHOLD HERE\n",
    "        flux_plot.append(ul[j])\n",
    "        uplim_bool.append(1)\n",
    "        flux_err_plot.append(2e-6)\n",
    "        \n",
    "    else:\n",
    "        flux_plot.append(flux[j])\n",
    "        uplim_bool.append(0)\n",
    "        flux_err_plot.append(flux_err[j])\n",
    "    j = j + 1\n",
    "    \n",
    "uplims = np.array(uplim_bool, dtype = bool)\n",
    "\n",
    "low_xerr = []\n",
    "k = 0\n",
    "\n",
    "while k < len(tcen):\n",
    "    a = tcen[k] - tmin[k]\n",
    "    low_xerr.append(a)\n",
    "    k = k + 1\n",
    "high_xerr = low_xerr\n",
    "    \n",
    "xerr = [low_xerr, high_xerr]\n",
    "\n",
    "fig = plt.subplots(figsize=(14, 8))\n",
    "plt.errorbar(tcen, flux_plot, xerr = xerr, yerr = flux_err_plot,\n",
    "             uplims = uplims, linestyle = 'None', color = 'k',\n",
    "             marker = 'None')\n",
    "plt.title('NGC 1275 Light-Curve')\n",
    "plt.xlabel('Time [MJD]')\n",
    "plt.ylabel('Flux MeV cm^(-2) s^(-1)')\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Time (MJD)\")\n",
    "plt.ylabel(\"Flux ($MeV cm^{-2} s^{-1}$)\")\n",
    "plt.savefig('NGC_1275_lc', dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look!\n",
    "\n",
    "![lc](../img/NGC_1275_lc.png)\n",
    "\n",
    "By eye, that looks variable! NGC 1275 is a relatively bright $\\gamma$-ray source, and is known for exhibiting variability. During past states of high gamma-ray activity it's been possible to achieve 12-hour bin sizes, which is quite impressive for an object of this nature. Again, we can fit a curve to this data, but for now we are done!\n",
    "\n",
    "Congratulations, you have just executed a more or less complete analysis of a radio galaxy using NASA's *Fermi* $\\gamma$-ray space telescope! \n",
    "\n",
    "![welldone](../img/welldone.jpg)\n",
    "\n",
    "If you are ready for more of a challenge, then you can look at the Crab Nebula. The Crab Nebula consists of a pulsar wind nebula (4FGL J0534.5+2201e) and its central pulsar both are spatially coincident with each other, however the Crab is on the galactic plane, so this is more of a challenge to model. \n",
    "\n",
    "To look at the Crab, you will need to download a dataset for it (following the instructions in tutorial 1) and then write a **YAML** configuration file (you can copy and change the NGC 1275 one) and execute an analysis chain using the ***fermipy*** library. We recommend setting up a separate directory to do this in. The methods should be largely the same. We've studied the Crab as a project before, so if you have any issues, please let us know and we can try and help. Good Luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
